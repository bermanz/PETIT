\begin{abstract}
  The task of image-to-image (I2I) translation involves transforming the style of an image to that of a desired modality while preserving its content.
  In most cases, pairs of content-wise-aligned input-target images are not available, resulting in an unpaired I2I (UI2I) setup.
  Due to the lack of input-target pairs, the UI2I task is ill-posed, resulting in numerous equally satisfactory outputs for a single input.
  However, some data domains have unique properties that tie the input to the output; these can be exploited to reduce this ambiguity.
  We propose PETIT-GAN, a physically enhanced thermal image-translating generative adversarial network to perform UI2I translation between thermal images.
  Our novel approach embeds physically modeled prior information in an UI2I translation to produce outputs with greater fidelity to the target modality.
  We further show that our solution outperforms the current state-of-the-art models at thermal UI2I translation by approximately $50\%$ with respect to the standard metrics, and enjoys a significantly more consistent training procedure.
\end{abstract}
