{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd() / 'src'))\n",
    "\n",
    "from src.configs import TestConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from dataset import MonoDS\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "config = TestConfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Configure and initialize the model to be tested according to the desired specifications:\n",
    "1. Model backbone (default is CUT).\n",
    "2. Use the camera intrinsic (FPA) temperature (default is True).\n",
    "3. Use the physical model (default is True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycle_gan_model import CycleGANModel\n",
    "from cut_model import CUTModel\n",
    "from utils.deep import NetPhase\n",
    "\n",
    "\n",
    "# config.model = \"CUT\" # \"CycleGan\"\n",
    "# config.thermal.is_fpa_input = True\n",
    "# config.thermal.is_physical_model = True\n",
    "\n",
    "\n",
    "backbone = CUTModel if config.model == \"CUT\" else CycleGANModel\n",
    "model = backbone(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint\n",
    "The model object loads the following types of weights:\n",
    "- D: the weights of the discriminator.\n",
    "- G: the weights of the deep generator.\n",
    "- F: the weights of the contrastive classifier network (for CUT backbone only).\n",
    "- PW: the weights of the affine layer applied to the physical estimator.\n",
    "- coefficients: the calibrated coefficients of the physical estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_phase(NetPhase.test)\n",
    "model.setup(config)\n",
    "model.load_networks(\"best\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataset = MonoDS(src_dir=Path(\"data\", \"pan\", \"test\"))\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=int(config.data_loader.num_threads),\n",
    ")\n",
    "print(f\"The number of images for inference = {len(dataloader) * batch_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "The loop transforms the panchromatic test set to synthetic monochromatic images and saves them either as .npy (original resolution) or .png (uint8) formats according to the user specification of the fmt_of_output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_of_output = \"png\"\n",
    "\n",
    "path_to_save = Path.cwd() / 'results' / 'transformed' / datetime.now().strftime(\"%Y%m%d_h%Hm%Ms%S\")\n",
    "path_to_save.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, data in enumerate(tqdm(dataloader, desc=\"Test\")):\n",
    "        model.set_input(data)\n",
    "        model.forward()\n",
    "\n",
    "        visuals = model.get_current_visuals()\n",
    "\n",
    "        for type in [\"pan_real\", \"mono_fake\", \"mono_phys\"]:\n",
    "            sub_dir = path_to_save / type\n",
    "            sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "            cur_vis = visuals[type]\n",
    "            domain = type.split(\"_\")[0]\n",
    "            images = model.rec_image(cur_vis, domain=domain, fmt=fmt_of_output)\n",
    "            if fmt_of_output == \"npy\":\n",
    "                for j, image in enumerate(images):\n",
    "                    np.save(f\"{str(sub_dir)}/{i*batch_size + j}.npy\", image)\n",
    "            else:\n",
    "                for j, image in enumerate(images):\n",
    "                    pil_img = Image.fromarray(image)\n",
    "                    pil_img.save(f\"{str(sub_dir)}/{i*batch_size + j}.{fmt_of_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
